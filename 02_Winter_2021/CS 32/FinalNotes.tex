\documentclass[class=article, crop=false]{standalone}
\input{~/github/latex/Preamble}

\fancyhf{}
\lhead{CS32 Final Notes}
\rhead{Page \thepage}
\pagestyle{fancy}

\begin{document}
  \tableofcontents
  \newpage
  \section{Runtime Complexity}
  \textbf{Main Idea.} We give algorithms a ``rating''(read: Big-O notation) based on how many instructions the algorithm takes as a function of the size of the input data. We always consider the \emph{worst case scenario}.
  \begin{note}{}
    Always give the Big-O notation in terms of the variables that you're given, i.e. if the size of the array is $k$, then your Big-O notation should be $O(k)$, \emph{not} $O(n)$.
  \end{note}
  \subsection{Single Input Algorithms}
  To get the Big-O of an algorithm, we only consider the most commonly performed operations, and ignore the rest. In other words, take the term with the highest degree and remove its coefficient. \par
  When you know $n$ is small, forget which Big-O is better and choose the algorithm that is \emph{easiest} to program, as it won't make that much of a difference anyways.
  \subsection{Multi-input Algorithms}
  If an algorithm operates on two (or more) \emph{independent} data sets, your Big-O must include \emph{all} variables in the final notation, i.e. $O(c^2 + e)$.
  \subsection{STL Runtime Complexity}
  \subsubsection{Vector} 
  \textbf{Purpose:} A resizable array
  \begin{itemize}
    \item Inserting an item (top/middle): $O(n)$
    \item Inserting an item (bottom): $O(1)$
    \item Delete an item (top/middle): $O(n)$
    \item Delete an item (bottom): $O(1)$
    \item Access an item (anywhere): $O(1)$
    \item Finding an item: $O(n)$
  \end{itemize}
  \subsubsection{List} 
  \textbf{Purpose:} A linked list
  \begin{itemize}
    \item Inserting an item (top/middle*/bottom): $O(1)$
    \item Deleting an item (top/middle*/bottom): $O(1)$
    \item Access an item (top/bottom): $O(1)$
    \item Access an item (middle): $O(n)$
    \item Finding an item: $O(n)$
  \end{itemize}
  \begin{note}{*}
    To get to the middle, you may have to first iterate through $X$ items, at cost $O(X)$.
  \end{note}
  \subsubsection{Set} 
  \textbf{Purpose:} Maintains a set of unique items
  \begin{itemize}
    \item Inserting an item (anywhere): $O(\log_2n)$
    \item Deleting an item (anywhere): $O(\log_2n)$
    \item Finding an item: $O(\log_2n)$
  \end{itemize}
  \subsubsection{Map} 
  \textbf{Purpose:} Maps one item to another
  \begin{itemize}
    \item Inserting an item (anywhere): $O(\log_2n)$
    \item Deleting an item (anywhere): $O(\log_2n)$
    \item Finding an item: $O(\log_2n)$
  \end{itemize}
  \subsubsection{Queue and Stack}
  \textbf{Purpose:} Classic stack/queue
  \begin{itemize}
    \item Inserting an item (anywhere): $O(1)$
    \item Popping an item (anywhere): $O(1)$
    \item Examining the top: $O(1)$
  \end{itemize}
  \section{Sorting Algorithms I: Inefficient Sorts}
  \begin{definition}{Stable/Unstable Sorting}
    An \emph{unstable} sorting algorithm re-orders the items without taking into account their initial ordering, whereas a \emph{stable} sorting algorithm takes into account the initial ordering, maintaining the order of similarly-valued items.
  \end{definition}
  \subsection{Selection Sort}
  \begin{itemize}
    \item Look at all $N$ items, and find the ``smallest'' one
    \item Swap that item with the first item (the smallest item is in the front)
    \item Look at the remaining $N-1$ items
    \item Swap that item with the second item
    \item Repeat...
  \end{itemize}
  Because you need $N$ steps to find the ``smallest item'', and then $N-1$ steps to find the second ``smallest'' item, etc., in total this algorithm runs in $O(N^2)$ time. If the items are already mostly sorted, selection sort still takes just as many steps. Selection sort is also unstable.
  \subsection{Insertion Sort}
  \begin{itemize}
    \item Look at the first two items, and swap them if they are out of order
    \item Now look at the third item, and insert it into its proper spot, moving things around as necessary
    \item Repeat until the entire shelf has been sorted
  \end{itemize}
  Because you need $1$ step to sort the first two books, then $2$ steps for the first three, etc., this algorithm runs in $O(N^2)$ time. However, if the books are already sorted, then insertion sort doesn't need to move anything around, so it runs in $O(N)$ time. The worst case scenario is when the items are perfectly mis-ordered. Insertion sort is also stable.
  \subsection{Bubble Sort}
  \begin{itemize}
    \item Look at the first two items, and swap if they're out of order
    \item Look at the next two items, and swap if they're out of order
    \item Repeat until you hit the end of the array (now the largest element is at the end)
    \begin{itemize}
      \item If at least one swap was performed, start back at the top and repeat the process again
    \end{itemize}
  \end{itemize}
  Because you need to pass through the list of $N$ elements $N$ times, this algorithm runs in $O(N^2)$ time. Just like insertion sort, this algorithm is really efficient on pre-sorted arrays and linked lists. Bubble sort is also stable.
  \begin{note}{Characteristics of Sorts}
    After one round of selection sort, the smallest item is at the front. After one round of bubble sort, the largest item is at the end. After one round of insertion sort, the first two items are in the correct order.
  \end{note}
  \section{Sorting Algorithms II: Electric Boogaloo}
  \subsection{Quicksort}
  \begin{itemize}
    \item If the array has only zero or one element, return
    \item Select an arbitrary element $P$ from the array (typically the first)
    \item Move all elements that are less than or equal to $P$ to the left of $P$, and all elements greater than $P$ to the right of $P$ 
    \item Recursively repeat this process on the left and right sub-arrays
  \end{itemize}
  \begin{note}{}
    The ``pivot'' $P$ will be in the correct position after doing quicksort once, as all the items to its left are smaller than $P$, and all the items to its right are greater than $P$.
  \end{note}
  Because partitioning the array costs $N$ steps, and we do this $\log_2N$ times (as we cut the array in half each time), this algorithm runs in $O(N\log_2N)$ time. If the array is already (mostly) sorted, then the left sub-array is empty and the right sub-array contains $N-1$ items, and we need to repeat $N$ times, so the runtime complexity becomes $O(N^2)$. Quicksort is also unstable.
  \subsection{Merge Sort}
  \begin{itemize}
    \item Sort the left and right sub-arrays
    \item Compare the next items in the left and right sub-arrays, and copy over the smaller one
    \item Repeat...
    \begin{itemize}
      \item If there are no more items left in one sub-array, copy over all of the remaining items in the other sub-array
    \end{itemize}
  \end{itemize}
  Because merging takes $N$ steps to complete, and we are continuously dividing our pile in half each time, we need to run this process $\log_2N$ times. Thus the runtime complexity of this algorithm is $O(N\log_2N)$. Merge sort runs efficiently regardless of how the items are originally arranged, but takes up more space because it needs to store an auxiliary list. Merge sort is also stable.
  \section{Trees}
  \textbf{Main Idea.} Trees are a data structure that stores values in a hierarchical fashion. We often use linked lists to build trees (each vertex is a node with pointers), and trees can be used as an alternative to arrays and linked lists for storing data.
  \begin{definition}{Binary Tree}
    A \emph{binary tree} is a tree in which each node has \emph{at most} two children nodes: a left child and a right child.
  \end{definition}
  \subsection{Traversals}
  \begin{itemize}
    \item Pre-order Traversal: Process the current node, then traverse the left and right subtrees
    \item In-order Traversal: Traverse the left subtree, process the current node, then traverse the right subtree
    \item Post-order Traversal: Traverse the left and right subtrees, then process the current node
    \item Level-order Traversal: Traverse the nodes level by level, starting from the root node and working your way down
  \end{itemize}
  An easy way to remember the order of the traversals is to start at the root node and draw a loop counter-clockwise around all of the nodes. Pre-order will process a node everytime you pass the \emph{left} side of it, In-order will process a node everytime you pass \emph{below} it, Post-order will process a node everytime you pass the \emph{right} side of it. Level-order traversal will go row by row.
  \subsection{Binary Search Trees}
  \begin{definition}{Binary Search Tree (BST)}
    A \emph{binary search tree} is a binary tree such that for every node $X$ in the tree,
    \begin{itemize}
      \item All nodes in $X$'s left subtree must be less than $X$
      \item All nodes in $X$'s right subtree must be greater than $X$
    \end{itemize}
  \end{definition}
  To find an item in a binary search tree, we only need to take $\log_2N$ steps, because each step we take effectively removes half of the tree. In the worst case scenario, only left nodes or only right nodes are used, and it takes $N$ steps to find an item. The runtime complexity for printing out all the items in a tree or freeing all the nodes in a tree is $O(N)$, because the algorithm needs to visit all $N$ nodes in the tree. \par
  To free all the items in a tree, we need to do a post-order traversal, to make sure that we delete all of the children before deleting the current node.
  \subsubsection{BST Node Insertion}
  \begin{itemize}
    \item If the root is null, create a new node with the given value and make root point to it
    \item Otherwise, compare the given value with the current node's value
    \item If the given value is less than the current node's value, go to the left child (if it exists) and repeat. If it doesn't create a new node with the given value and make the current node's left child the new node.
    \item If the given value is greater than the current node's value, go to the right child (if it exists) and repeat. If it doesn't create a new node with the given value and make the current node's right child the new node.
  \end{itemize}
  \subsubsection{BST Node Deletion}
  There are three cases to handle when deleting a node from a binary tree:
  \begin{enumerate}
    \item If the node is a leaf, set the parent's pointer to the leaf to be null and then delete the target node.
    \item If the node has one child, relink the parent node to the target node's only child, then delete the target node.
    \item If the node has two children, replace the target node with either the largest node in the left subtree, or the smallest node in the right subtree.
  \end{enumerate}
  \subsection{Huffman Encoding}
  Count the frequencies of every character in your text file, and save it into a list. Then to create a Huffman tree, we perform the following process:
  \begin{itemize}
    \item Take the two items with the lowest frequency off of the bottom of the list, and make them the children of a parent node (whose frequency is the sum of its children)
    \item Add this back into the list
    \item Repeat...
  \end{itemize}
  When you only have one item left in your list of frequencies, that becomes the root node of your Huffman tree. To get the bit representation of a character in the Huffman tree, just trace a path from the root node to that character in the tree. Start with an empty string, and everytime you go down the left branch, add a $0$ to your string, and everytime you go down the right branch, add a $1$ to your string. The end result is how you would encode that character using Huffman encoding.
  \subsection{Balanced Search Trees}
  The only thing you need to know is that balanced BSTs are always $O(\log_2N)$ for insertion and deletion. When encountering BSTs in job/internship interviews, ask if the BST is balanced.
  \section{Hash Tables}
  \textbf{Main Idea.} Hash tables are often \emph{the most efficient} way to search for data. \\
  The goal here is to have a more efficient way to find data than a BST (which is $\log_2N$). One way we could do this is to just have a massive boolean array and store a \texttt{true} in the index corresponding to the value we want, but this is very space inefficient. An alternative way to do this is to map our items to a smaller range of numbers using the modulus operator.
  \subsection{Collision Avoidance}
  \begin{definition}{Collision}
    A \emph{collision} is a condition where two or more values both map to the same bucket in the array.
  \end{definition}
  There are two ways to get around collisions: linear probing insertion and the open hash table.
  \subsection{Linear Probing}
  To insert an item into the closed hash table, first check if the target bucket is empty. If it is, store the value into the bucket. If it isn't, just keep moving down the array until you find an empty slot. If you go past the end of the array, loop back up to the top of the array. \\[10pt]
  To search our hash table, we use a similar approach. We first compute a target bucket number using our mapping function and then look in that bucket for our value. If we find it, great! Otherwise, probe linearly down the array until we either find our value or hit an empty bucket. If you run into an empty bucket, it means your value isn't in the array. \\[10pt]
  Deleting an item in a closed hash table is garbage, so we want to use an ``open hash table''.
  \subsection{Open Hash Table}
  \textbf{Idea.} Instead of storing our values directly in the array, each array bucket points to a linked list of values. \\[10pt]
  As before, compute a bucket number using your mapping function. Search the linked list at \texttt{array[bucket]} for your item. If you reach the end of the linked list, the item is not in the table. To delete an item from the hash table, just remove the item from the linked list.
  \subsection{Hash Table Efficiency}
  \textbf{Question.} How large must our hash table be so that it runs quickly?
  \begin{definition}{Load Factor}
    The \emph{load} of a hash table is the maximum number of values you intend to add divided by the number of buckets in the array.
    \[
      L = \frac{\text{Max number of values to insert}}{\text{Total number of buckets}}
    \]
    The load tells you what portion of your buckets are actually going to be utilized.
  \end{definition}
  \begin{note}{}
    Open hash tables are almost \emph{always} more efficient than closed hash tables.
  \end{note}
  The average number of tries it'll take for you to insert/find an item in a hash table is
  \begin{align*}
    \underbrace{\frac{1}{2}\paren{1 + \frac{1}{1-L}} \text{ for }L < 1.0}_{\text{Closed Table}} && \underbrace{1 + \frac{L}{2}}_{\text{Open Table}}
  \end{align*}
  \begin{note}{}
    When choosing the exact size of your hash table, always try to make it a prime number for more distribution and even fewer collisions.
  \end{note}
  \subsection{A Hash Function for Strings}
  We could use our own hash function for mapping strings to integers, but we're lazy. Let's use the one provided by C++ by using \texttt{\#include <functional>}. \\[10pt]
  We can create a string hashing object with \texttt{std::hash<std::string> str\_hash}, and then get a value out of it with \texttt{unsigned int hashValue = str\_hash(hashMe)} (between $0$ and $4$ billion). We can then apply our own modulo function and return a bucket number that fits into our hash table's array.
  \subsection{Creating a Custom Hash Function}
  If we are creating a hash function for our own data type, we want it to satisfy a few criteria:
  \begin{enumerate}
    \item The hash function must always give us the same output for a given input value
    \item The hash function should disperse items throughout the hash array as randomly as possible
    \item When coming up with a new hash function, always measure hw well it disperses items
  \end{enumerate}
  \subsection{Hash Tables versus Binary Search Trees}
  \begin{center}\begin{tabular}{c|c|c}
    & Hash Tables & Binary Search Trees \\
    \hline
    Speed & $O(1)$ regardless of the number of items & $O(\log_2N)$ \\
    \hline
    Simplicity & Easy to implement & More complex to implement \\
    \hline
    Max Size & Closed: Limited by array size & Unlimited size \\
             & Open: Not limited, but high load & \\
             & impacts performance & \\ 
    \hline
    Space efficiency & Wastes a lot of space if you have & Only uses as much memory as needed \\
                     & a large hash table holding few items & (one node per item inserted) \\
    \hline
    Ordering & No ordering (random) & Alphabetical ordering
  \end{tabular}\end{center}
  \section{Tables}
  \textbf{Main Idea.} Tables are the building block of databases---they are very useful for organizinig large amounts of data and making it quickly searchable.
  \begin{definition}{Table Definitions}
    A group of related data in a table is called a \emph{record}. Each record has a number of \emph{fields} that can be filled with values. A number of these records is what we call a \emph{table}.
  \end{definition}
  \subsection{Implementing Tables}
  We can just use a struct or class to hold the values for each record, and then an array or vector to hold our records. \\[10pt]
  To make this efficient, we add a new data structure that lets us associate each person's field types with their slot number in the vector, i.e. name with position, ID number with position, phone number with position, etc. Each of these secondary data structures is called an \emph{index}, and helps us efficiently find a record based on a particular field.
  \begin{note}{}
    Remember that any time you delete or update a record's searchable fields, you also you to update your indexes.
  \end{note}
  While hash tables are more efficient than BSTs, BSTs can be more useful if you need to process elements in order, i.e. if you need to print out the names in order.
  \section{Priority Queues}
  \textbf{Main Idea.} Useful for prioritizing different types of data for processing based on their importance.
  \begin{definition}{Priority Queue}
    A \emph{priority queue} is a special type of queue that allows us to keep a prioritized list of items. In such a queue, each item you insert has a ``priority rating'' that indicates how important it is. In contrast to a regular queue, when you dequeue an item from a priority queue, it dequeues the item with the \emph{highest priority}, instead of the first item inserted.
  \end{definition}
  A priority queue has three operatons:
  \begin{itemize}
    \item Inserting a new item into the queue
    \item Getting the value of the highest priority item
    \item Removing the highest priority item from the queue
  \end{itemize}
  For a limited set of priorities, we can use $n$ linked lists, one for each priority level (i.e. high, medium, low). If we have a large number of priorities, we can use a \emph{heap} data structure, which is a special kind of binary tree.
  \begin{note}{}
    While a heap uses a binary tree to store its data, it is \emph{not} a binary search tree.
  \end{note}
  \begin{definition}{Complete Binary Tree}
    A binary search tree is \emph{complete} if the top $n-1$ levels of the tree are completely fileld with nodes, and all nodes on the bottom-most level must be as far left as possible (with no empty slots between nodes).
  \end{definition}
  \subsection{Types of Heaps}
  There are two kinds of heaps, \emph{minheaps} and \emph{maxheaps}:
  \begin{itemize}
    \item Maxheap:
    \begin{itemize}
      \item Quickly insert an item into the heap
      \item Quickly retrieve the \emph{largest} item from the heap
    \end{itemize}
    \item Minheap:
    \begin{itemize}
      \item Quickly insert an item into the heap
      \item Quickly retrieve the \emph{smallest} item from the heap
    \end{itemize}
  \end{itemize}
  \subsection{The Maxheap}
  A \emph{maxheap} is a binary search tree satisfying:
  \begin{enumerate}
    \item The value contained by a node is \emph{always greater than or equal to} the values of the node's children
    \item The tree is a \emph{complete} binary tree
  \end{enumerate}
  Notice that by definiton, the largest (highest priority) item is always at the root of the tree.
  \subsubsection{Extracting the Largest Item}
  \begin{enumerate}
    \item If the tree is empty, return error
    \item Otherwise, the top item in the tree has the largest value (save it for later)
    \item If the heap has only one node, delete it and return the saved value
    \item Copy the value from the right-most node in the bottom-most row to the root node
    \item Delete the right-most node in the bottom-most row
    \item Repeatedly swap the just-moved value with the larger of its two children until the value is greater than or equal to both of its children (this is called \emph{sifting down})
    \item Return the saved value to the user
  \end{enumerate}
  \subsubsection{Adding a Node}
  \begin{enumerate}
    \item If the tree is empty, create a new root node and return
    \item Otherwise, insert the new node in the bottom-most, left-most position of the tree (so it's still a complete tree)
    \item Compare the new value with its parent's value
    \item If the new value is greater than its parent's value, then swap them
    \item Repeat steps 3-4 until the new value rises to its proper place
  \end{enumerate}
  \begin{note}{}
    This process is also known as \emph{reheapification}.
  \end{note}
  \subsection{How to Implement a Heap}
  We use an array to store our tree, row by row. The root node goes in the first slot of the array, the next two nodes in the next two slots, etc. \\[10pt]
  Some properties of our array-based tree are:
  \begin{enumerate}
    \item We can always find the root node in \texttt{heap[0]}
    \item We can always find the bottom-most, right-most node in \texttt{heap[count - 1]}
    \item We can always find the bottom-most, left-most node in \texttt{heap[count]}
    \item We can add or remove a node by simply setting \texttt{heap[count] = value;} and/or updating our count
  \end{enumerate}
  To find the indices for the left and right children of a node, we have
  \[
    \mathrm{leftChild}(\text{parent}) = 2\cdot \text{parent} + 1 \quad \text{and}\quad \mathrm{rightChild}(\text{parent}) = 2\cdot \text{parent} + 2.
  \]
  To find the parent of a child node, we have
  \[
    \frac{\text{child} - 1}{2} = \text{parent}.
  \]
  \begin{note}{}
    The above only works if you use integer division.
  \end{note}
  Because our tree is a complete binary tree, it will have a height of $\log_2N$. Inserting and extracting requires ``bubbling'' the items up and down the tree, so inserting/extracting all $N$ items takes $2N\log_2N$ steps, so the runtime complexity is $O(N\log_2N)$.
  \section{Heapsort}
  A na\"ive way to do this is to first insert all of the items into a heap and then extract them all, but this runs in $O(N\log_2N)$ time.
  \subsection{Efficient Heapsort}
  Given an array of $N$ numbers that we want to sort:
  \begin{enumerate}
    \item Convert our input array into a maxheap
    \item While there are numbers left in the heap:
    \begin{enumerate}[label=(\alph*)]
      \item Remove the biggest value from the heap
      \item Place it in the last open slot of the array
    \end{enumerate}
  \end{enumerate}
  To make this more efficient, start checking at the parent of the right-most node in the bottom-most row to avoid checking for all of the single-node subtrees. \\[10pt]
  The runtime complexity of heapsort is $O(N\log_2N)$ because converting the array to a heap takes $N$ steps, and extracting the items takes $N\log_2N$ steps.
  \section{Graphs}
  \begin{definition}{Graph}
    A \emph{graph} is an ADT that stores a set of entities and also keeps track of the relationships between all of them.
  \end{definition}
  There are two main kinds of graphs, \emph{directed} and \emph{undirected} graphs. Edges in directed graphs can only go on one direction, while edges in undirected graphs can go in either direction. \\[10pt]
  The easiest way to represent a graph is with a double-dimensional array. The size of both dimensions of the array is equal to the number of vertices in the graph, and each element in the array indicates whether or not there exists an edge between vertex $i$ and vertex $j$. This is called an \emph{adjacency matrix}. Furthermore, if you raise the adjacency matrix to the $n$th power, you get which vertices are $n$ edges apart.
  \subsection{Another Way to Represent a Graph}
  We can also use an array of $n$ linked lists (an \emph{adjacency list}) to represent a graph. If we add a number $j$ to the $i$th list, this means there is an edge from vertex $i$ to vertex $j$.
  \subsection{Which Representation to Use?}
  Use an adjacency matrix if you have lots of edges between vertices but few vertices ($<10,000$ vertices). Use an adjacency list if you have few edges between vertices and lots of vertices ($>10,000$ vertices). A graph that has many edges is called a \emph{dense graph}, and one with few edges is called a \emph{sparse} graph.
  \subsection{Graph Traversals}
  There are two main types of graph traversals (both of which we've seen before): \emph{depth-first traversal} and \emph{breadth-first traversal}. The former employs a stack, and the latter a queue. 
  \subsection{Weighted Graphs}
  \begin{definition}{Weighted Graph Definitions}
    A \emph{weighted graph} is a graph where each edge connected vertices $u$ and $v$ has a \emph{weight} or \emph{cost} associated with it. The \emph{weight} of a path from $u$ to $v$ is the sum of the weights of the edges between the two vertices. The \emph{shortest path} between two vertices is the path with the lowerst total cost of edges between the two vertices.
  \end{definition}
  \subsection{Finding a Shortest Path}
  To find a shortest path, we use Dijkstra's Algorithm. It relies on the basic idea of \emph{settled vertices} (we know the minimal distance to the start) and \emph{unsettled vertices} (we don't know the minimal distance to the start). The algorithm proceeds as follows:
  \begin{itemize}
    \item Set all vertices to have a distance of $-\infty$
    \item Let the start vertex $A$ have a value of $0$
    \item Let all of $A$'s neighbors have a tentative value of the weight of the edge connecting $A$ and the vertex
    \item Moving to the neighbor of $A$ with the smallest value, repeat this process (this vertex is now settled)
    \begin{itemize}
      \item If you find a path to a vertex that is less than the vertex's original value, update the table with the lower value
    \end{itemize}
  \end{itemize}
  To actually implement this in code, we perform the same algorithm with two arrays---an integer array holding the lengths of the minimal paths from the start vertex to that vertex, and a boolean array that holds whether each vertex is ``settled'' or not.
\end{document}

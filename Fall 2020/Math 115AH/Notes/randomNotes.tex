\documentclass[class=article, crop=false]{standalone}
\input{~/github/latex/Preamble}

\fancyhf{}
\lhead{Kyle Chui}
\rhead{Page \thepage}
\pagestyle{fancy}

\begin{document}
  \begin{theorem}{Similar Matrices $\iff$ Same Transformation, Different Bases (self-named)}
    Let $A, B\in \M_n(F)$. Then $A\sim B$ if and only if there exists a vector space over $F$, $\dim(V) = n$, $T\colon V\to V$ linear and ordered bases $\mathcal{B}$, $\mathcal{C}$ for $V$ such that
    \[
      A = [T]_{\mathcal{B}}\quad \text{and} \quad B = [T]_{\mathcal{C}}.
    \]
    In other words, $A\sim B$ if and only if they represent the same linear transformation relative to (possibly) different ordered bases.
  \end{theorem}
  \begin{definition}{Eigenstuffs}
    Let $0\neq V$ be a vector space over $F$, $T\colon V\to V$ a \emph{linear operator} (a linear transformation where the domain = target) and $\lam\in F$. Set
    \[
      S_\lam\ceq T - 1_V\colon V\to V
    \]
    a linear operator, so
    \[
      S_\lam(v) = Tv - \lam v \tag{For all $v\in V$}
    \]
    We say $\lam$ is an \emph{eigenvalue} of $T$ if $S_\lam$ is \emph{not} injective ($\ker(S_\lam)\neq 0$).
    Let
    \[
      E_T(\lam)\ceq \ker(S_\lam) = \set{v\in V\mid Tv - \lam v = 0} = \set{v\in V\mid Tv = \lam v}.
    \]
    If $E_T(\lam)\neq 0$, we call $E_T(\lam)$ an \emph{eigenspace} of $V$ relative to $T$, $\lam$, and any vector $v\in E_T(\lam)$ is called an \emph{eigenvector} of $T$ relative to $\lam$. So if $T\colon V\to V$ is linear, $\lam\in F$ is an eigenvalue of $T$ if and only if there exists some non-zero vector $v\in V$ such that $Tv = \lam v$.
  \end{definition}
  \begin{note}{Eigenspaces are Subspaces}
    Eigenspaces are just kernels, so they are subspaces.
  \end{note}
  \begin{definition}{Characteristic Polynomial}
    Let $A\in \M_n(F)$. Define
    \[
      f_A = \det(tI - A)\in F[t],
    \]
    called the \emph{characteristic polynomial} of $A$.
  \end{definition}
  \textbf{Proposition.} If $A, B\in \M_n(F)$ are similar, then $f_A = f_B$.
  \begin{proof}
    If $A = C\inv BC$, where $C$ is invertible, then 
    \begin{align*}
      f_A &= \det(tI - A) \\
          &= \det(C\inv(tI)C - C\inv BC) \\
          &= \det(C\inv (tI - B)C) \\
          &= \det(C\inv)\det(tI-B)\det(C) \\
          &= \det(tI-B) \\
          &= f_B.
    \end{align*}
  \end{proof}
  \begin{theorem}{14.3}
    Let $V$ be a finite-dimensional vector space over $F$, and let $T\colon V\to V$ be linear. Then the eigenvalues of $T$ are precisely the roots of $f_T$.
  \end{theorem}
  \begin{theorem}{Cayley-Hamilton Theorem}
    Let $A\in \M_n(F)$. Then $f_A(A) = 0$. In other words, you get $0$ when you plug $A$ into the expansion of the determinant $f_A$. \par
    \textbf{WARNING!!!} You \emph{must} prove this in order to use this!!!
  \end{theorem}
  \begin{definition}{Minimal Polynomial}
    Let $V$ be a finite-dimensional vector space over $F$, $T\colon V\to V$ linear. You will show on your take home exam that there exists a polynomial $q\in F[t]$ such that
    \begin{enumerate}[label=(\alph*)]
      \item $q\neq 0$
      \item $q(A) = 0$, $A = [T]_{\mathcal{B}}$, where $\mathcal{B}$ is an ordered basis for $V$
      \item $\deg g$ is the minimal degree for a polynomial $g\neq 0$ in $F[t]$ to satisfy $g(A) = 0$
      \item $q$ is \emph{monic}, in other words it has leading coefficient one
    \end{enumerate}
    Moreover, $q$ is unique and called the \emph{minimal polynomial} of $A$ and is denoted $q_T$. Using it shows a stronger form of the Cayley-Hamilton Theorem, written below.
  \end{definition}
  \begin{theorem}{Cayley-Hamilton (Stronger Form)}
    Let $V$ be a finite-dimensional vector space over $F$, $T\colon V\to V$ linear, then $q_T\mid f_T$ in $F[t]$. In other words, there exists some $g\in F[t]$ such that $f_T = q_T\cdot g$.
  \end{theorem}
  \begin{theorem}{}
    Let $T\colon V\to V$ be linear and $\lam_1,\dotsc,\lam_n$ in $F$ distinct eigenvalues of $T$, $0\neq v_i\in E_T(\lam_i)$, $i = 1,\dotsc,n$. Then $\set{v_1,\dotsc,v_n}$ is linearly independent.
  \end{theorem}
  \begin{proof}
    We induct on $n$. Observe that for $n=1$, we have that $v_1\neq 0$, so it is linearly independent. Suppose that the statement holds for some $n = k-1$. \par
    Suppose that
    \[
      0 = \alpha_1v_1 + \dotsb + \alpha_nv_n. \tag{$\alpha_1,\dotsc,\alpha_n\in F$}
    \]
    We apply the linear operator $S_{\lam_n} = T - \lam_n1_v$ to the above equation. Then
    \[
      S_{\lam_n} = T_{v_i} - \lam_nv_i = \lam_iv_i - \lam_nv_i = (\lam_i - \lam_n)v_i,
    \]
    so we have
    \begin{align*}
      0 &= \alpha_1S_{\lam_{v_n}}v_1 + \dotsb + \alpha_nS_{\lam_{v_n}}v_n \\
        &= \alpha_1(\lam_1-\lam_n)v_1 + \dotsb + \alpha_n(\lam_n-\lam_n)v_n \\
        &= \alpha_1(\lam_1-\lam_n)v_1 + \dotsb + \alpha_{n-1}(\lam_{n-1}-\lam_n)v_{n-1}.
    \end{align*}
    By the induction hypothesis, $v_1,\dotsc,v_{n-1}$ are linearly independent, so the coefficients must be zero. Furthermore, because $\lam_1,\dotsc,\lam_n$ are distinct, we know that $\lam_i-\lam_n\neq 0$. Thus we have $\alpha_i = 0$ for $i = 1,\dotsc,n-1$. Finally, because $v_n\neq 0$, we have $\alpha_n = 0$ as well. Thus $\set{v_1,\dotsc,v_n}$ is linearly independent.
  \end{proof}
  \begin{note}{}
    The theorem above also works for the infinite case, i.e. $\lam_i\in F$ for $i\in I$, an infinite indexing set.
  \end{note}
  \begin{note}{Alternate Proof}
    Again we consider the equation 
    \[
      0 = \alpha_1v_1 + \dotsb + \alpha_nv_n. \tag{$\alpha_1,\dotsc,\alpha_n\in F$}
    \]
    Applying $T$ to this equation and multiplying it by $\lam_n$ yield
    \begin{align*}
      0 &= \alpha_1\lam_1v_1 + \dotsb + \alpha_n\lam_nv_n\,\,\, \quad\text{and} \\
      0 &= \alpha_1\lam_nv_1 + \dotsb + \alpha_n\lam_nv_n, \quad\text{respectively.}
    \end{align*}
    Thus we can subtract the two equations to get
    \[
      0 = \alpha_1(\lam_1 - \lam_n)v_1 + \dotsb + \alpha_n(\lam_n - \lam_n)v_n,
    \]
    which is the same result that we had above.
  \end{note}
  \textbf{Corollary.} Let $V$ be a finite-dimensional vector space over $F$, $\dim(V)= n$. If $T\colon V\to V$ linear has $n$ distinct eigenvalues, then $T$ is diagonalizable.
  \begin{note}{}
    The above corollary is rare, and its converse is \emph{false}.
  \end{note}
  \textbf{Corollary 2.} If $V$ is a finite-dimensional vector space over $F$, $\dim(V)=n$, $T\colon V\to V$ linear, then $T$ has at \emph{most} $n$ eigenvalues. \par
  \textbf{Corollary 3.} Let $V$ be a vector space over $F$, $T\colon V\to V$ linear, $\lam_1,\dotsc,\lam_n$ distinct eigenvalues of $T$. Set 
  \[
    W = E_T(\lam_1) + \dotsb + E_T(\lam_n).
  \]
  If $v_i\in E_T(\lam_i)$, $i=1,\dotsc,n$ satisfy
  \[
    v_1+\dotsb+v_n = 0,
  \]
  then $v_i = 0$ for $i = 1,\dotsc,n$. We write this as
  \[
    W = E_T(\lam_1) \oplus \dotsb \oplus E_T(\lam_n).
  \]
  \textbf{Exercise.} Let $V$ be a vector space over $F$, $W_1,\dotsc,W_n\sse V$ subspaces. Let $W = W_1+\dotsb+W_n$. Then the following are equivalent.
  \begin{enumerate}[label=(\alph*)]
    \item If $w_i\in W_i$ for $i=1,\dotsc,n$ satisify $w_1+\dotsb+w_n=0$, then $w_i = 0$ for all $i$. We say the $W_i$ are \emph{independent}. This is a generalisation of linear independence.
    \item If $v\in W$, there exists a unique $w_i\in W_i$ such that $v = w_1+\dotsb+w_n$. This is a generalisation of the Coordinate Theorem.
    \item $W_i\cap \sum\limits_{\substack{j\neq i\\j = 1}}^n W_j = 0$ for all $i = 1,\dotsc,n$.
    \item If $\mathcal{B}_i$ is a basis for $W_i$, $i = 1,\dotsc,n$, then $\mathcal{B} = \mathcal{B}_1\cup\dotsb\cup\mathcal{B}_n$ is a basis for $W$.
  \end{enumerate}
  If these hold for $W$, then we say $W$ is an \emph{(internal) direct sum} of the $W_i$ and write $W = W_1\oplus\dotsb\oplus W_n$. \par
  \textbf{Exercise.} Let $V$ be a vector space over $F$, $W_1,\dotsc,W_n\sse V$ subspaces such that $V = W_1+\dotsb+W_n$. Let
  \[
    W = W_1\times \dotsb\times W_n = \set{(w_1,\dotsc,w_n)\mid w_i\in W_i \text{ for all }i}
  \]
  a vector space over $F$ via component wise operations. Show that $V = W_1\oplus\dotsb\oplus W_n$ if and only if 
  \[
    T\colon W_1\times \dotsb\times W_n\to V \quad\text{by}\quad (w_1,\dotsc,w_n)\mapsto w_1+\dotsb+w_n
  \]
  is an isomorphism. We call $W$ the \emph{(external) direct sum} of the $W_i$. \par
  \textbf{Consequence.} Let $V$ be a finite-dimensional vector space over $F$, $\lam_1,\dotsc,\lam_n$ distinct eigenvalues of $T\colon V\to V$ linear, $r_i = \dim(E_T(\lam_i))$, $\mathcal{B}_i$ ordered basis for $E_T(\lam_i)$, where $i = 1,\dotsc,n$. If
  \[
    V = E_T(\lam_1) + \dotsb + E_T(\lam_n),
  \]
  then
  \[
    V = E_T(\lam_1) \oplus \dotsb \oplus E_T(\lam_n),
  \]
  and $\mathcal{B} = \mathcal{B}_1\cup\dotsb\cup \mathcal{B}_n$ is an ordered basis for $V$ (obvious order) and
  \[
    [T]_{\mathcal{B}} = \begin{bmatrix}[\lam_1 1_{E_T(\lam_1)}]_{\mathcal{B}_1} & \cdots & 0 \\ \vdots & \ddots & \vdots\\ 0 & \cdots & [\lam_n 1_{E_T(\lam_n)}]_{\mathcal{B}_n}\end{bmatrix},
  \]
  where $[\lam_i 1_{E_T(\lam_i)}]_{\mathcal{B}_i}$ is a $r_i\times r_i$ matrix. We call this \emph{block form}, and it is a diagonal matrix. In particular, 
  \[
    f_T = \det(t 1_V - T) = (t - \lam_1)^{r_1}(t - \lam_n)^{r_n}
  \]
  and $T$ is diagonalizable. By determinant theory (assumed), we have $\det \begin{pmatrix}A&0\\0&B\end{pmatrix} = \det(A)\det(B)$ where $A,B$ are square matrices.
  \section{Inner Product Spaces}
  When talking about inner product spaces, we shall always assume that our field $F$ lies in $\C$ (e.g. $\Q, \R, \C$ as a subfield). \par
  Let $\bar{\phantom i}\colon \C\to\C$ be defined by $a + \beta\sqrt{-1}\mapsto \alpha-\beta\sqrt{-1}$ for all $\alpha,\beta\in \R$. We call this \emph{complex conjugation}. Then
  \begin{enumerate}[label=(\alph*)]
    \item $a = \bar{a}$ if and only if $a\in \R$
    \item $\bar{\bar{a}} = a$
    \item $\abs{a}^2\ceq a\bar{a}\geq 0$ in $\R$ as $a\bar{a}=\alpha^2+\beta^2$ and $\abs{a}^2 = 0$ if and only if $a=0$
  \end{enumerate} 
  As we are assuming $F\sse \C$, we define
  \[
    \bar{F} \ceq \set{\bar{z}\in\C\mid z\in F} \tag{Is a subfield}
  \]
  and we shall also assume that $F = \bar{F}$.
  \begin{note}{}
    This is always true if $F\sse \R$ or $F = \C$, but does not always hold unless we only consider those $F$ that do (which we will).
  \end{note}
  \begin{definition}{Inner Product}
    Let $F\sse \C$ be a subfield satisfying $F = \bar{F}$, $V$ a vector space over $F$. We call $V$ an \emph{inner product space over $F$} under the map
    \[
      \tup{,}\ceq \tup{,}_V\colon V\times V\to F.
    \]
    We write $\tup{v,w}$ for $\tup{,}(v, w)$ if $\tup{,}$ satisfies the following for all $v_1,v_2,v_3,v\in V$, for all $\alpha\in F$.
    \begin{enumerate}[label=(\alph*)]
      \item $\tup{v_1+v_2,v_3} = \tup{v_1,v_3} + \tup{v_2,v_3}$
      \item $\tup{v_1,v_2}=\overline{\tup{v_2,v_1}}$
      \item $\tup{\alpha v_1,v_2} = \alpha \tup{v_1,v_2} = \tup{v_1,\bar{\alpha}v_2}$
      \item $\tup{v,v}\in \R$ and $\tup{v,v}\geq 0$ with $\tup{v,v} = 0$ if and only if $v = 0$
    \end{enumerate}
    If $V$ is an inner product space over $F$ (under $\tup{,}$), the \emph{length} (or \emph{norm} or \emph{magnitude}) of $v\in V$ is given by 
    \[
      \norm{v}\ceq \sqrt{\tup{v,v}}\geq 0 \quad \text{in }\R \tag{non-negative square root}
    \]
  \end{definition}
  \begin{note}{}
    If $F < \C$, $\norm{v}^2\in F$, but it is possible that $\norm{v}\notin F$. For example, if $V = \Q^2$ a vector space over $\Q$ and an inner product space over $\Q$, under the dot product we have $\norm{\tup{1,1}} = \sqrt{2}\notin \Q$.
  \end{note}
  \subsection{Properties of Inner Product Spaces}
  Let $V$ be an inner product space over $F$, $\alpha\in F$, $v_1,v_2,v_3\in V$.
  \begin{enumerate}[label=(\alph*)]
    \item $\tup{0,v}=0=\tup{w,0}$ for all $v,w\in V$
    \item 
    \begin{enumerate}[label=\roman*.]
      \item $\tup{\alpha v_1+v_2,v_3} = \alpha \tup{v_1,v_3}+\tup{v_2,v_3}$
      \item $\tup{v_1, \alpha v_2 + v_3} = \bar{\alpha}\tup{v_1,v_2}+ \tup{v_1,v_3}$
    \end{enumerate}
    \item If $F\sse\R$, define the angle $\theta$, $0\leq \theta < 2\pi$ between $v_1\neq 0$ and $v_2\neq 0$ in $V$ by
    \[
      \cos\theta\ceq \frac{\tup{v_1,v_2}}{\norm{v_1}\norm{v_2}} \tag{$\norm{v_1},\norm{v_2}\neq 0$ because $v_1,v_2\neq 0$}
    \]
    and if $F\not\sse\R$ define $\theta$ by
    \[
      \cos\theta\ceq \frac{\abs{\tup{v_1,v_2}}}{\norm{v_1}\norm{v_2}}.
    \]
    \textbf{Note.} This does not make sense yet, and will not until we show that $\frac{\abs{\tup{v_1,v_2}}}{\norm{v_1}\norm{v_2}}\leq 1$ for $v_1,v_2\neq 0$.
    \item Let $v\in V$. If $\tup{v,w}=0$ for all $w\in V$ (or $\tup{w,v} = 0$ for all $w\in W$), then $v=0$.
    \item Let $0\neq x\in V$. Then
    \[
      \tup{,x}\colon V\to F \quad \text{by}\quad v\mapsto \tup{v, x}
    \]
    is a linear transformation (and linear functional, so $\tup{,x}\in V^*$). However, 
    \[
      \tup{x,}\colon V\to F \quad \text{by}\quad v\mapsto \tup{x, v}
    \]
    is linear if and only if $F\sse\R$. 
    \begin{definition}{Sesquilinearity}
      Let $V, W$ be inner product spaces over $F$ and $f\colon V\to W$ is a map. We call $f$ a \emph{sesquilinear} map (or transformation) if 
      \[
        f(\alpha v + v') = \bar{\alpha}f(v) + v' \quad \text{for all }v,v'\in V, \text{for all }\alpha\in F.
      \] 
    \end{definition}
    In general, we say that $\tup{x,}$ is \emph{sesquilinear} as for all $\alpha\in F$, for all $v_1,v_2\in V$, we have
    \[
      \tup{x,\alpha v_1+v_2} = \bar{\alpha}\tup{x, v_1} +\tup{x, v_2}.
    \]
  \end{enumerate}
  \begin{example}{Inner Product Spaces}
    \begin{enumerate}[label=(\alph*)]
      \item Let $V = F^n$ and $\tup{,} =$ the dot product. In other words, if $v = (\alpha_1,\dotsc,\alpha_n)$ and $w = (\beta_1,\dotsc,\beta_n)$ where $\alpha_i,\beta_i\in F$ for all $i, j$, then 
      \[
        \tup{v, w} = \sum_{i=1}^{n}\alpha_i \bar{\beta_i}.
      \]
      Note that if $F\sse \R$, then 
      \[
        \tup{v, w} = \sum_{i=1}^{n}\alpha_i \beta_i.
      \]
      \item Let $I = [\alpha, \beta]$, $\alpha < \beta$ in $\R$, $V = C(I)$ with $C(I) = \set{f\colon I\to \R\mid f \text{ is continuous}}$. Then 
      \[
        \tup{f, g}\ceq \int_{\alpha}^{\beta}fg. \tag{$fg = f(x)g(x)$}
      \]
      \item \phantom{}
      \begin{definition}{Adjoint Matrix}
        Let $A\in \M_n(F)$. Define the \emph{adjoint} of $A$ to be $A^*$ where
        \[
          (A^*)_{ij}\ceq \bar{A}_{ji} \quad\text{for all }i, j,
        \]
        the \emph{conjugate transpose} of $A$, i.e. $A^* = \bar{A}^t$. Notice that if $F\sse \R$, $A^* = A^t$.
      \end{definition}
      Let $V = \M_n(F)$ under
      \[
        \tup{A, B}\ceq \tr(AB^*)
      \]
      where $\tr C = \sum_{i=1}^{n}C_{ii}$. Notice that if $F\sse\R$, then $\tup{A, B} = \tr(AB^t)$.
      \item Let $F = \R$, and define
      \[
        \ell_2\ceq \set{(a_0,a_1,\dotsc,a_n,\dotsc)\mid a_i\in\R \text{ for all }i, \sum a_i^2 < \infty}.
      \]
      It is a vector space over $\R$ by component wise operations (a subspace of $\R^\infty_{\text{inf}}$---see below) and an inner product space over $\R$ via
      \[
        \tup{v, w}\ceq \sum_{i=0}^{\infty}a_ib_i \quad \text{in }\R,
      \]
      where $v = (a_0,a_1,\dotsc)$ and $w = (b_0,b_1,\dotsc)$.
    \end{enumerate}
  \end{example}
  \begin{theorem}{}
    Let $V$ be an inner product space over $F$. Then for all $v_1,v_2\in V$, for all $\alpha\in F$, we have
    \begin{enumerate}[label=(\alph*)]
      \item $\norm{v_1}\in \R$ with $\norm{v_1}\geq 0$, and $\norm{v_1} = 0$ if and only if $v_1 = 0$
      \item $\norm{\alpha v_1} = \abs{\alpha}\norm{v_1}$
      \item Cauchy-Schwarz Inequality: $\abs{\tup{v_1,v_2}} \leq \norm{v_1}\norm{v_2}$. Note that this makes angles between vectors wel defined.
      \begin{proof}
        If $v_1 = 0$ or $v_2 = 0$, the result is immediate, so we may assume that $v_1, v_2\neq 0$. We use a trick from calculus (orthogonal projection) to prove this. Let 
        \[
          v = v_2 - \underbrace{\frac{\tup{v_2,v_1}}{\norm{v_1}^2}v_1}_{\mathclap{\text{Projection onto }v_1}}.
        \]
        We claim that $\tup{v, \alpha v_1} = 0$ for all $\alpha\in F$. Then 
        \begin{align*}
          \tup{v, \alpha v_1} &= \tup{v_2 - \frac{\tup{v_2, v_1}}{\norm{v_1}^2}v_1, \alpha v_1} \\
                              &= \tup{v_2, \alpha v_1} + \tup{-\frac{\tup{v_2,v_1}}{\norm{v_1}^2}v_1, \alpha v_1} \\
                              &= \bar{\alpha}\tup{v_2, v_1} -\frac{\tup{v_2,v_1}}{\norm{v_1}^2}\tup{v_1, \alpha v_1} \\
                              &= \bar{\alpha}\tup{v_2,v_1} - \frac{\tup{v_2,v_1}}{\norm{v_1}^2}\bar{a}\tup{v_1,v_1} \\
                              &= \bar{\alpha}\tup{v_2,v_1} - \tup{v_2,v_1}\bar{\alpha} \\
                              &= 0.
        \end{align*}
        Furthermore, we have
        \begin{align*}
          0\leq \tup{v,v} &= \tup{v, v_2 - \frac{\tup{v_2, v_1}}{\norm{v_1}^2}v_1} \\
                          &= \tup{v, v_2} + \tup{v_1, \frac{\tup{v_2, v_1}}{\norm{v_1}^2}v_1} \\
                          &= \tup{v, v_2} \\
                          &= \tup{v_2 - \frac{\tup{v_2, v_1}}{\norm{v_1}^2}v_1, v_2} \\
                          &= \tup{v_2,v_2} + \tup{-\frac{\tup{v_2, v_1}}{\norm{v_1}^2}v_1, v_2} \\
                          &= \norm{v_2}^2 - \frac{\tup{v_2,v_1}}{\norm{v_1}^2}\tup{v_1,v_2} \\
                          &= \norm{v_2}^2 - \frac{\overline{\tup{v_1,v_2}}}{\norm{v_1}^2}\tup{v_1,v_2} \\
                          &= \norm{v_2}^2 - \frac{\abs{\tup{v_1,v_2}}^2}{\norm{v_1}^2}.
        \end{align*}
        Rearranging, we have $\abs{\tup{v_1,v_2}}^2\leq \norm{v_1}^2\norm{v_2}^2$ so $\abs{\tup{v_1,v_2}}\leq \norm{v_1}\norm{v_2}$.
      \end{proof}
      \item Minkowski Inequality (Special Case): $\norm{v_1 + v_2}\leq \norm{v_1} + \norm{v_2}$.
      \begin{proof}
        Observe that
        \begin{align*}
          \norm{v_1+v_2}^2 &= \tup{v_1+v_2, v_1+v_2} \\
                           &= \norm{v_1}^2 + \tup{v_1,v_2} + \tup{v_2,v_1} + \norm{v_2}^2 \\
                           &= \norm{v_1}^2 + \tup{v_1,v_2} + \overline{\tup{v_1,v_2}} + \norm{v_2}^2. \\
          \intertext{By Cauchy-Schwarz, we have}
                           &\leq (\norm{v_1}+\norm{v_2})^2.
        \end{align*}
        Therefore $\norm{v_1+v_2}\leq \norm{v_1}+\norm{v_2}$.
      \end{proof}
    \end{enumerate}
  \end{theorem}
  \begin{definition}{Metric Space}
    If $V$ is an inner product space over $F$, we define the \emph{distance} between $v_1, v_2$ in $V$ by
    \[
      d(v_1, v_2)\ceq \norm{v_1-v_2}\geq 0 \quad \text{in }\R.
    \]
    Then for all $v, w, x\in V$, $d$ satisfies:
    \begin{enumerate}[label=(\alph*)]
      \item $d(v, w)\geq 0$ in $\R$ and $d(v, w) = 0$ if and only if $v = w$
      \item $d(v, w) = d(w, v)$ 
      \item $d(v, x)\leq d(v, w) + d(w, x)$ (Triangle Inequality)
    \end{enumerate}
    We call $V$ a \emph{metric space} under $d$ if $d$ satisfies all of the above.
  \end{definition}
  \subsection{Orthogonal Bases}
  \begin{definition}{Orthostuffs}
    Let $V$ be an inner product space over $F$, $\es\neq S\sse V$ a subset. We say
    \begin{enumerate}[label=(\alph*)]
      \item $S$ is \emph{orthogonal} if $\tup{v, w} = 0$ for all $v\neq w$ in $S$.
      \item If $S$ is an orthogonal set, we call it \emph{orthonormal} if $\norm{v} = 1$ for all $v\in S$.
      \item An orthogonal set is called an orthogonal basis if it is both a basis and orthogonal (same for orthonormal).
      \item If $v, w\in V$ we say $v, w$ are \emph{orthogonal} or \emph{perpendicular} if $\tup{v, w} = 0$. We write $v\perp w$. Note that this is equivalent to if $\tup{w, v} = 0$.
    \end{enumerate}
  \end{definition}
  \begin{theorem}{}
    Let $V$ be an inner product space over $F$, $S\sse V$ an orthogonal set. Suppose that $0\notin S$. Then $S$ is linearly independent. If, in addition, $V$ is a finite-dimensional inner product space over $F$ and $\abs{S} = \dim(V)$, then $S$ is an orthogonal basis for $V$.
  \end{theorem}
  \textbf{Crucial Equation.} If $\set{v_1, \dotsc,v_n}$ is an orthogonal set, $v_i\neq 0$ for all $i$ and
  \[
    v = \alpha_1v_1 + \dotsb + \alpha_nv_n, \tag{$\alpha_1, \dotsc,\alpha_n\in F$}
  \]
  then 
  \[
    \alpha_j = \frac{\tup{v, v_j}}{\norm{v_j}^2}. \tag{$j = 1,\dotsc,n$}
  \]
  This comes from taking the inner product with $v_j$ on both sides of the first equation.\par
  \textbf{Corollary.} (Parseval's Equation) Let $V$ be a finite-dimensional inner product space over $F$ with orthonormal basis $\set{v_1, \dotsc,v_n}$ and $v, w\in V$. Then
  \[
    \tup{ v, w} = \sum_{i=1}^{n}\tup{v, v_i}\overline{\tup{w, v_i}}.
  \]
  In particular, 
  \[
    \norm{v}^2 = \sum_{i=1}^{n}\abs{\tup{v,v_i}}^2.
  \]
  \begin{theorem}{Gram-Schmidt Theorem}
    Let $V$ be an inner product space over $f$ and $\es\neq S_n = \set{v_1,\dotsc,v_n}\sse V$ a linearly independent set. Then there exist $y_1,\dotsc,y_n\in V$ such that
    \begin{enumerate}[label=(\alph*)]
      \item $y_1 = v_1$
      \item $T_n = \set{y_1,\dotsc,y_n}$ is an orthogonal set and linearly independent
      \item $\vspan(T_n) = \vspan(S_n)$
    \end{enumerate}
    The proof of this is basically Replacement Theorem + orthogonal projection. We construct $T_n$ from $S_n$ using the \emph{Gram-Schmidt process}.
  \end{theorem}
  \begin{theorem}{Orthogonal Theorem}
    Let $V$ be a finite-dimensional inner product space over $F$. Then $V$ has an orthogonal basis. If $F = \R$ or $\C$, then $V$ has an orthonormal basis.
  \end{theorem}
  \begin{definition}{Gamma Function}
    This is a generalization of binomial coefficients. The \emph{gamma function} $\Gamma(z)$, $z$ a complex variable, is defined by 
    \[
      \Gamma(z)\ceq \int_{0}^{\infty}x^{z-1}e^{-x} \dx x,
    \]
    with $\mathrm{Re}(z) > 0$. When $n\in \Z^+$, we have that $\Gamma(n) = (n-1)!$.
  \end{definition}
  \subsection{Orthogonal Complements}
  \textbf{Notation.} We let $F\sse \C$ be a field satisfying $F = \bar{F}$.
  \begin{definition}{Distance From a Vector to a Subset}
    Let $V$ be an inner product space over $F$, $v_1,v_2\in V$. We know that the \emph{distance} between $v_1,v_2$ is defined to be
    \[
      d(v_1,v_2)\ceq \norm{v_1-v_2}.
    \]
    More generally, let $\es\neq S\sse V$ be a subset and $v\in V$. Define the \emph{distance} of $v$ to $S$ by 
    \[
      d(v, S)\ceq \inf \set{d(v, w)\mid w\in S} \tag{$\inf$ is the infimum}
    \]
    if it exists and hence finite.
  \end{definition}
  \begin{definition}{Orthogonality to a Subset}
    Let $V$ be an inner product space over $F$, $\es\neq V\sse V$ a subset, $v\in V$. We say $v$ is \emph{orthogonal} (or \emph{perpendicular}) to $S$, writing $v\perp S$ if 
    \[
      \tup{s, v} = 0 \quad\text{for all } s\in S. 
    \]
    The set
    \[
      S^\perp\ceq \set{v\in V\mid v\perp S}
    \]
    is called the \emph{orthogonal complement} of $S$ in $V$.
  \end{definition}
  \begin{theorem}{Decomposition Theorem}
    Let $V$ be an inner product space over $F$, $S\sse V$ a finite-dimensional subspace, $v\in V$. Then there exists a unique $s\in S$, $s^\perp\in S^\perp$, such that $v = s + s^\perp$. In other words, you can uniquely decompose any vector $v\in V$ into components in $S$ and $S^\perp$.\par
    Moreover, if $v = s + s^\perp$ where $s\in S$, $s^\perp\in S^\perp$, then
    \[
      \norm{v}^2 = \norm{s}^2 + \norm{s^\perp}^2. \tag{Pythagorean Theorem}
    \]
    If, in addition, $V$ is a finite-dimensional inner product space over $F$, then
    \[
      \dim(V) = \dim(S) + \dim(S^\perp)
    \]
  \end{theorem}
  \section{Orthogonal Polynomials}
  
\end{document}

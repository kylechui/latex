\documentclass[class=article, crop=false]{standalone}
\input{../../Preamble}

\fancyhf{}
\lhead{Kyle Chui}
\rhead{Page \thepage}
\pagestyle{fancy}

\begin{document}
  \section{Lecture 17}
  \subsection{Bivariate Distributions of the Discrete Type}
  \begin{definition}{Joint Probability Mass Function}
    Let $X, Y$ be a pair of discrete random variables taking values in sets $S_X, S_Y\in\R$.
    \begin{itemize}
      \item We think of $(X, Y)$ as being a random point in $\R^2$ taking values in the set
      \[
        S = S_X\times S_y = \set{(x, y)\mid x\in S_X\text{ and }y\in S_y}.
      \]
      \item We define the \emph{joint probability mass function} of $X, Y$ to be the function $p_{X, Y}\colon S\to [0, 1]$ by
      \[
        p_{X, Y}(x, y) = \P[X = x, Y = y] = \P[(X, Y) = (x, y)].
      \]
    \end{itemize}
  \end{definition}
  \begin{theorem}{}
    Let $X, Y$ be discrete random variables taking values in sets $S_X, S_Y\sse\R$ and let $S = S_X\times S_Y$. If $X, Y$ have a joint PMF $p_{X, Y}(x, y)$ and $A\sse\R^2$ then
    \[
      \P[(X, Y)\in A] = \sum_{(x, y)\in A\cap S}p_{X, Y}(x, y).
    \]
  \end{theorem}
  \begin{theorem}{}
    Let $X, Y$ be discrete random variables taking values in sets $S_X, S_Y\sse\R$ and let $S = S_X\times S_Y$. If $X, Y$ have joint PMF $p_{X, Y}(x, y)$ then
    \[
      \sum_{(x, y)\in S}p_{X, Y}(x, y) = 1.
    \]
    \begin{proof}
      Observe that
      \[
        1 = \P[(X, Y)\in\R^2] = \sum_{(x, y)\in S\cap \R^2} p_{X, Y}(x, y) = \sum_{(x, y)\in S}p_{X, Y}(x, y).
      \]
    \end{proof}
  \end{theorem}
  \begin{definition}{Marginal Probability Mass Function}
    Let $X, Y$ be discrete random variables taking values in $S_X, S_Y\sse\R$.
    \begin{itemize}
      \item We define the \emph{marginal probability mass function of $X$} to be the function $p_X\colon S_X\to [0, 1]$ given by
      \[
        p_X(x) = \P[X = x].
      \]
      \item We define the \emph{marginal probability mass function of $Y$} to be the function $p_Y\colon S_Y\to [0, 1]$ given by
      \[
        p_Y(y) = \P[Y = y].
      \]
    \end{itemize}
  \end{definition}
  \begin{theorem}{}
    Let $X, Y$ be discrete random variables taking values in sets $S_X, S_Y\sse\R$. Let $X, Y$ have joint PMF $p_{X, Y}(x, y)$. Then,
    \[
      p_X(x) = \sum_{y\in S_Y} p_{X, Y}(x, y)\quad\text{and}\quad p_Y(y) = \sum_{x\in S_X} p_{X, Y}(x, y)
    \]
  \end{theorem}
  \begin{definition}{Independence}
    Let $X, Y$ be discrete random variables taking values in sets $S_X, S_Y\sse\R$ and let $S = S_X\times S_Y$.
    \begin{itemize}
      \item We say that random variables $X, Y$ are \emph{independent} if the events $\set{X = x}$ and $\set{Y = y}$ are independent for all $(x, y)\in S$.
      \item Equivalently, we have
      \[
        p_{X, Y}(x, y) = p_X(x)p_Y(y)\quad\text{for all}\quad(x, y)\in S.
      \]
    \end{itemize}
  \end{definition}
\end{document}
